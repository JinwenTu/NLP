{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fundamentals of Text Pre-processing\n",
    "\n",
    "Text data always appears in an unstructured form. It has to be manipulated and converted into a proper structured and numerical format consumable by text analysis algorithms, which is referred to as text pre-processing. It is an important task and a critical step in text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accessing Various Text Resources\n",
    "What are the text corpora and lexical resources often used in text analysis? Where and how can we \n",
    "access them? \n",
    "Text data used for different text analysis tasks can be derived from various resources, such as \n",
    "* **Existing data repositories**, most of which contains corpora that have been either pre-processed into a specific format that can be directly digested by the downstream text analysis algorithms or manually annotated. \n",
    "For example,\n",
    "    *  [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table) contains 30 corpora that can be used in text mining tasks, such as regression, clustering, and classification.  \n",
    "    * [Linguistic Data Consortium](https://www.ldc.upenn.edu/) contains corpora mainly used in various natural language processing tasks, such as parsing, acoustic analysis, phonological analysis and etc. One disadvantage of using LDC is that its corpora are not free. Users have to buy a license in order to use those corpora.\n",
    "* **NLTK**: A language toolkit that also includes a diverse set of corpora and lexical resources, which include, for example,\n",
    "    * Plain text corpora, e.g.,\n",
    "        * The Gutenberg Corpus contains thousands of books.\n",
    "    * Tagged Corpora, e.g.,\n",
    "        * The Brown Corpus is annotated with part-of-speech tags. Each word is now paired with its part-of-speech tag.\n",
    "           You can retrieve words as (word, tag) tuples, rather than just bare word strings.\n",
    "    * Chunked Corpora, e.g.,\n",
    "        * The CoNLL corpora includes phrasal chunks (CoNLL 2000), named entity chunks (CoNLL 2002).\n",
    "    * Parsed Corpora, e.g.,\n",
    "        * The Treebank corpora provide a syntactic parse for each sentence, like the Penn Treebank based on Wall Street Journal samples.\n",
    "    * Word List and Lexicons, e.g.,\n",
    "        * [WordNet](https://wordnet.princeton.edu/): a large lexical database of English, where nouns, verbs, adjectives and adverbs are organized into interlinked synsets (i.e., sets of synonyms)\n",
    "    * Categorized Corpora: \n",
    "        * The Reuters corpus: a corpus of Reuters News stories for used in developing text analysis algorithms.\n",
    "* **Web**: The largest source for getting text data is the Web. Text can be extracted from webpages or be retrieved\n",
    "via various APIs. For example,\n",
    "     * Wikipedia articles: The Wikimedia website provides links to download dumps of Wikipedia articles. Click [here](https://dumps.wikimedia.org/enwiki/20160305/) to view various dumps for English Wikipedia articles. \n",
    "     * Tweets that allows people to communicate with short, 140-characters messages. It is fortunate that Twitter provides quite well documented API that we can use to retrieve tweets of our interest. We will cover more about retrieving and pre-processing tweets in chapter 3.\n",
    "\n",
    "The set of NLTK corpora can be easily accessed with interfaces offered by NLTK. You can find a good introduction \n",
    "in [8] and a tutorial in [9]. Please do read these two activities. Here we show you how to install the text data that comes with NLTK and all the packages included in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above two commands gives you a window which allows you to browse the available corpora and packages included in NLTK. Once the downlaod finishes, close the window. Now you are ready to continue.\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Steps of Pre-Processing Text\n",
    "The possible steps of text pre-processing are nearly the same for all text analysis tasks, though which pre-processing \n",
    "steps are chosen depends on the specific task. The basic steps are as follows:\n",
    "* Tokenization\n",
    "* Case normalization\n",
    "* Removing Stop words\n",
    "* Stemming and Lemmatization\n",
    "* Sentence Segmentation\n",
    "\n",
    "\n",
    "### 2.1. Tokenization\n",
    "\n",
    "Text is usually represented as sequences of characters by computers. \n",
    "However, most natural language processing (NLP) and text mining tasks need to operate on tokens. \n",
    "The process of breaking a stream of text into tokens is often referred to as **tokenization**.\n",
    "For example, a tokenizer turns a string such as \n",
    "```\n",
    "    A data wrangler is the person performing the wrangling.\n",
    "```\n",
    "into a sequence of tokens such as\n",
    "```\n",
    "    \"A\" \"data\" \"wrangler\" \"is\" \"the\" \"person\" \"performing\" \"the\" \"wrangling\"\n",
    "```\n",
    "\n",
    "There is no single right way to do tokenization. \n",
    "It completely depends on the corpus and the text analysis task you are going to perform. \n",
    "In this section, we will demonstrate the process of chopping character sequences into pieces with different tokenizers. \n",
    "\n",
    "\n",
    "####  Standard Tokenizer\n",
    "For English, a straightforward tokenization strategy is to use white spaces as token delimiters. \n",
    "The whitespace tokenizer simply splits the text on any sequence of whitespace, tab, or newline characters.\n",
    "Consider the following hypothetical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"The GSO finace group in  U.S.A. provided Cole with about\n",
    "US$40,000,555.4 in funding, which accounts for 35.3% of Cole's revenue (i.e., AUD113.3m), \n",
    "as the ASX-listed firm battles for its survival.\n",
    "Mr. Johnson said GSO's recapitalisation meant \"the current shares are worthless\".\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a starting point, let's tokenize the text above by using any whitespace characters as token delimiters.\n",
    "As mentioned, these characters include whitespace (' '), tab ('\\t'), newline ('\\n'), return ('\\r'), and so on.\n",
    "We will use '\\s' rather than writing it as something like '[ \\t\\n]+'.\n",
    "\n",
    "There are multiple ways of tokenizing a string with whitespaces.\n",
    "The simplest approach might be using Python's string function split()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'GSO',\n",
       " 'finace',\n",
       " 'group',\n",
       " 'in',\n",
       " 'U.S.A.',\n",
       " 'provided',\n",
       " 'Cole',\n",
       " 'with',\n",
       " 'about',\n",
       " 'US$40,000,555.4',\n",
       " 'in',\n",
       " 'funding,',\n",
       " 'which',\n",
       " 'accounts',\n",
       " 'for',\n",
       " '35.3%',\n",
       " 'of',\n",
       " \"Cole's\",\n",
       " 'revenue',\n",
       " '(i.e.,',\n",
       " 'AUD113.3m),',\n",
       " 'as',\n",
       " 'the',\n",
       " 'ASX-listed',\n",
       " 'firm',\n",
       " 'battles',\n",
       " 'for',\n",
       " 'its',\n",
       " 'survival.',\n",
       " 'Mr.',\n",
       " 'Johnson',\n",
       " 'said',\n",
       " \"GSO's\",\n",
       " 'recapitalisation',\n",
       " 'meant',\n",
       " '\"the',\n",
       " 'current',\n",
       " 'shares',\n",
       " 'are',\n",
       " 'worthless\".']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\s+\", gaps=True)\n",
    "tokens = tokenizer.tokenize(raw)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `RegexpTokenizer` splits a string into tokens using a regular expression.\n",
    "Refer to its online [documentation](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer) \n",
    "for more details.\n",
    "Its constructor takes four arguments.\n",
    "\n",
    "In the example above, we used `\\s+` to match 1 or more whitespace characters.\n",
    "If the pattern defines separators between tokens, the value of `gaps` should be\n",
    "set to `True`. Otherwise, the pattern should be used to find the tokens.\n",
    "\n",
    "NLTK also provides a whitespace tokenizer, `WhitespaceTokenizer[source]`, which is\n",
    "equivalent to our tokenizer. Try\n",
    "```python\n",
    "    from nltk.tokenize import WhitespaceTokenizer\n",
    "    WhitespaceTokenizer().tokenize(raw)\n",
    "```\n",
    "\n",
    "It seems that word tokenization is quite simple if words in a language are all\n",
    "separated by whitespace characters. \n",
    "\n",
    "However, this is not the case in many languages other than English, such\n",
    "as Chinese, Japanese, Korean and Ancient Greek. \n",
    "In those languages, text is written without any whitespaces between words. \n",
    "So the whitespace tokenizer is of no use at all.\n",
    "To handle them, we need more advanced tokenization techniques, often referred to as\n",
    "word segmentation, which is an important and challenging task in NLP. \n",
    "However,\n",
    "discussing word segmentation is beyond our scope here.\n",
    "\n",
    "It is not surprising that the whitespace tokenizer is insufficient even for English, since English does not just contains sequences of alphanumeric characters separated by white spaces. \n",
    "It often contains punctuation, hyphen, apostrophe, and so on.\n",
    "Sometimes even whitespace does not necessarily indicate a word break. \n",
    "\n",
    "\n",
    "Back to our example, \n",
    "the whitespace tokenizer still gives us word like \"(i.e.,\", \"funding,\" and \"worthless\".\".\n",
    "We would like to remove parentheses, some punctuations, quotation marks and other non-alphanumeric characters.\n",
    "A simple and straightforward strategy is to use all non-alphanumeric characters as token delimiters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'GSO',\n",
       " 'finace',\n",
       " 'group',\n",
       " 'in',\n",
       " 'U',\n",
       " 'S',\n",
       " 'A',\n",
       " 'provided',\n",
       " 'Cole',\n",
       " 'with',\n",
       " 'about',\n",
       " 'US',\n",
       " '40',\n",
       " '000',\n",
       " '555',\n",
       " '4',\n",
       " 'in',\n",
       " 'funding',\n",
       " 'which',\n",
       " 'accounts',\n",
       " 'for',\n",
       " '35',\n",
       " '3',\n",
       " 'of',\n",
       " 'Cole',\n",
       " 's',\n",
       " 'revenue',\n",
       " 'i',\n",
       " 'e',\n",
       " 'AUD113',\n",
       " '3m',\n",
       " 'as',\n",
       " 'the',\n",
       " 'ASX',\n",
       " 'listed',\n",
       " 'firm',\n",
       " 'battles',\n",
       " 'for',\n",
       " 'its',\n",
       " 'survival',\n",
       " 'Mr',\n",
       " 'Johnson',\n",
       " 'said',\n",
       " 'GSO',\n",
       " 's',\n",
       " 'recapitalisation',\n",
       " 'meant',\n",
       " 'the',\n",
       " 'current',\n",
       " 'shares',\n",
       " 'are',\n",
       " 'worthless']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\W+\", gaps=True) \n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counterpart is to extract tokens that only consist of alphanumeric characters without the empty strings. Try the following out yourself:\n",
    "```python\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    tokenizer.tokenize(raw)\n",
    "```\n",
    "\n",
    "These two strategies are simple to implement, but there are cases where they may not match the desired behaviour. \n",
    "For example, \n",
    "\n",
    "the whitespace tokenizer - non-alphanumeric characters\n",
    "\n",
    "non-alphanumeric tokenizer - over-tokenise\n",
    "\n",
    "You will soon find that tokenizers should often be customized to deal with different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Periods in Abbreviations\n",
    "\n",
    "Word tokens are not always surrounded by whitespace characters. Punctuation, such as commas, semicolons, and periods, are often used in English, as they are vital to disambiguate the meaning of sentences. However, it is problematic for computers to handle punctuation, especially periods, properly in tokenization. \n",
    "In this part we will focus on the handling of periods.\n",
    "\n",
    "Periods are usually used to mark the end of sentences. Difficulty arises when the period marks abbreviations (including acronyms).  In the case of abbreviations, particularly acronyms, separating tokens on punctuation and other non-alphanumeric characters would put different components of the acronym into different tokens, as you have seen in our example, where \"U.S.A\" has been put into three tokens, \"U\", \"S\" and \"A\", losing the meaning of the acronym. \n",
    "\n",
    "An acronym is often formed from the initial components in multi-word phrases.  Some contains periods, and some do not. Common acronyms with periods are for example, \n",
    "* U.S.A\n",
    "* U.N.\n",
    "* U.K.\n",
    "* B.B.C\n",
    "\n",
    "For abbreviations like those, it is not hard to figure out the pattern and the corresponding regular expression.  Each of those abbreviations contains at least a pair of a letter (either uppercase or lowercase) and a period.  The regular expression is\n",
    "```python\n",
    "    r\"([a-zA-z]\\.)+\"\n",
    "```\n",
    "Put it into `RegexpTokenizer`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U.S.A.', 'i.e.', 'l.', 'r.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"(?:[a-zA-Z]\\.)+\") #?: match the max and return\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that\n",
    "1. We introduced `(?: )` in the regular expression to avoid just selecting substrings that match the pattern. To check out how `?:` affects the output, try to remove it and run the tokenizer again. You will get the following output\n",
    "```\n",
    "    ['e.', 'A.', 'l.', 'r.']\n",
    "```\n",
    "It just returns the last substrings that match the pattern.\n",
    "2. The code also returned 'l.' and 'r.' that are part of 'survival.' and 'Mr.' \n",
    "The period in 'survival.' marks the end of a sentence. \n",
    "Indeed, it is very challenging to deal with the period at the end of each sentence, as it can also be part of an abbreviation if the abbreviation appears at the end of a sentence.\n",
    "For example, the following sentence ends with 'etc.'\n",
    "```\n",
    "    I need milk, eggs, bread, etc.\n",
    "```\n",
    "\n",
    "Next, let’s further consider some more general abbreviations, like\n",
    "* Mr. and Mrs.\n",
    "* Dr.\n",
    "* st.\n",
    "* Wash. and Calif. (abbreviations for two states in U.S., Washington and California)\n",
    "\n",
    "In those abbreviations, the period is always preceded two or more letters in English alphabet. Turn this pattern into a regular expression\n",
    "```\n",
    "    r\"[a-zA-z]{2,}\\.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A.', 'e.', 'l.', 'r.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"([a-zA-Z]\\.)+\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['survival.', 'Mr.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"[a-zA-z]{2,}\\.\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not surprising that the ouput contains \"survival.\" again. \n",
    "Let's put all the cases together. \n",
    "The regular expression can be generalised to\n",
    "```python\n",
    "    r\"([a-zA-Z]+\\.)+\"\n",
    "```\n",
    "which matches both acronyms and abbreviations like \"Dr.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U.S.A.', 'i.e.', 'survival.', 'Mr.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"[a-zA-z]{2,}\\.|(?:[a-zA-Z]\\.)+\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned early in this chapter, the issues of tokenization are language specific.\n",
    "The language of the document to be tokenized should be known a priori.\n",
    "Take computer technology as an example.\n",
    "It has introduced new types of character sequences that a tokenizer should probably treat as a single token, including email addresses, web URLs, IP addresses, etc. One solution is to simply ignore them by using a non-alphanumeric-based tokenizer. \n",
    "However, this comes the cost of losing the original meaning of those kinds of tokens. For instance, if an IP address, like \"172.19.197.106\", is tokenized into individual numbers, \"172\", \"19\", \"197\", and \"106\".\n",
    "It is no longer an IP address, and these numbers can be anything.\n",
    "To account for strings like\n",
    "* \"172.19.197.106\"\n",
    "\n",
    "you can simply update our regular expression accounting for abbreviations to \n",
    "```python\n",
    "    (\\w+\\.?)+\n",
    "```\n",
    "\n",
    "Try it out on https://regex101.com/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Currency and Percentages\n",
    "While analysing financial document, such as finance reports, a financial analyst might be interested in monetary numerals mentioned in the reports. One interesting research question in both finance and computer science is whether one can use finance reports to help predict the stock market prices. In this case, it would be good for a tokenizer to keep all the monetary numerals.\n",
    "\n",
    "Currency is usually expressed in symbols and numerals (e.g., $10).\n",
    "There are many different ways of writing about different currencies.\n",
    "For example,\n",
    "* A three-letter currency abbreviations followed by figures, for example,\n",
    "```\n",
    "    AUD100, EUR500, CNY330 \n",
    "```\n",
    "\n",
    "* A letter or letters symbolising the country followed the, for example,\n",
    "```\n",
    "    A$100 (= AUD100), US$10 (= USD10), C$5 (= CAD5),\n",
    "```\n",
    "\n",
    "* A currency symbols ($, £, €, ¥, etc.) followed by figures, for examples\n",
    "```\n",
    "    £100.5, €30.0\n",
    "```\n",
    "\n",
    "While the number of digits in the integer part is more than three, commas are often inserted between every three digits, like\n",
    "```\n",
    "    AUD100, 000 \n",
    "```\n",
    "Let's construct a regular expression that can account for all the following monetary numerals\n",
    "```\n",
    "1. $10,000.00\n",
    "2. €10,000,000.00\n",
    "3. ¥5.5555\n",
    "4. AUD100\n",
    "5. A$10.555\n",
    "```\n",
    "The regular expression should looks like as follows:\n",
    "```python\n",
    "    r'''(?x)          \n",
    "        ([A-Z]{1,3})? # (1)\n",
    "        [\\$£€¥]?      # (2)\n",
    "        (\\d{1,3},)*   # (3)\n",
    "        \\d{1,3}       # (4)\n",
    "        (?:\\.\\d+)?    # (5)\n",
    "    '''\n",
    "```\n",
    "\n",
    "(1) matches the start of monetary numerals, which consists of one or up to 3 uppercase letters that indicate a country symbol or a currency abbreviation.\n",
    "<br/>\n",
    "(2) together with (1), matches the start of monetary numerals, which consists of either only a currency symbol or a country symbol plus a currency symbol.\n",
    "<br/>\n",
    "(3) accounts for the integer part that contains more than three digits. It matches all digits in the integer part except for the last three digits.\n",
    "<br/>\n",
    "(4) matches the last three digits in the integer part.\n",
    "<br/>\n",
    "(5) matches the fractional part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US$40,000,555.4', '35.3', 'AUD113.3']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"(?:[A-Z]{1,3})?[\\$£€¥]?(?:\\d{1,3},)*\\d{1,3}(?:\\.\\d+)?\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer back to our example text \"raw\", can you find any issue rather than the percentage (35.5%)? The regular expression cannot handle \"AUD113.3m\", where the \"m\" indicates million. Without 'm', the number 'AUD113.3' loses its meaning in the original context. Therefore, you have seen that there might not be a regular expression that can handle all possible ways of representing currency.\n",
    "\n",
    "Now, we have constructed a regular expression for currencies, even though it is not perfect.\n",
    "Next, we move to working out the regular expression for percentages, things becomes quite easy.\n",
    "Percentages usually have the following forms\n",
    "* 23%\n",
    "* 23.23%\n",
    "* 23.2323%\n",
    "* 100.00%\n",
    "\n",
    "The maximum number of digits in the integer part is 3, the minimun is 1, so the regular expression is '\\d{1,3}'.\n",
    "A percentage can have either one or no fractional part, which can be matched by '(\\.\\d+)?'.\n",
    "Adding % to the end, we have\n",
    "```python\n",
    "    r\"\\d{1,3}(\\.\\d+)%\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['35.3%']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\d{1,3}(?:\\.\\d+)?%\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code should give you the only percentage in our example text. \n",
    "Compare the regular expression matching percentages with that matching currency,\n",
    "you will find that the former is similar to the last bits of the latter, except for the percentage sign.\n",
    "Besides, there are other numerical and special expressions that\n",
    "we can not easily handle with regular expressions. For example, these expressions include\n",
    "email addresses, time, vehicle licence numbers, phone numbers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyphens and Apostrophes \n",
    "\n",
    "In English, hyphenation is used for various purposes. \n",
    "\n",
    "For example, if the hyphen is used to split up vowels in words, such as \"co-operate\", \"co-education\" and \"pre-process\", these words should be regarded as single token. In contrast, if the hyphen is used to group a couple of words together, for example, \"a state-of-the-art algorithm\" and \"a money-back guarantee\", these hyphenated words should be separated into individual words.\n",
    "Therefore, handling hyphenated words automatically is one of the most difficult tasks in pre-processing text data.\n",
    "\n",
    "The use of hyphens in many such cases is extremely inconsistent, you should be clear that there is no way of handling all the hyphen cases.\n",
    "\n",
    "Let's assume that we are going to treat all strings of two words separated by a hyphen as a single token. The pattern here is  a sequence of alphanumeric character plus \"-\" and plus another sequence of alphanumeric character.\n",
    "The corresponding regular expressions should be \n",
    "```python\n",
    "    r\"\\w+-\\w\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASX-listed']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+-\\w+\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to hyphens, how to handle an apostrophe in tokenization is another complex question. The apostrophe in English is often used in two cases:\n",
    "* Contractions: a shortened version of a word or multiple words. \n",
    "    * don't (do not)\n",
    "    * she'll (she will)\n",
    "    * you're (you are)\n",
    "    * he's (he is or he has)\n",
    "    * you'd (you would)\n",
    "* Possessives: used to indicate ownership/possession with nouns.\n",
    "    * the cat's tail\n",
    "    * Einstein's theory\n",
    "    \n",
    "Should we treat a string containing apostrophes as a single word or two words?\n",
    "Perhaps, you might think we should separate English Contractions into two words, and regard possessives as a single word. \n",
    "However, distinguishing contractions from possessives is not easy.\n",
    "For example, should \"cat's\" be \"cat has/is\" or the possessive case of cat.\n",
    "Thus some processor in NLP splits the strings in either case into two words, while others do not.\n",
    "Here we again assume that we are going to retrieve all strings with an apostrophe as single words.\n",
    "The regular expression is quite similar to the one for handling hyphens.\n",
    "```\n",
    "     r\"\\w+'\\w+\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Cole's\", \"GSO's\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+'\\w+\")\n",
    "tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generalise the `\\w+` to permit word-internal hyphens and apostrophes:\n",
    "```\n",
    "    \\w+(?:[-']\\w+)? \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Case Normalization\n",
    "After word tokenization, you may find that words can contain either upper- or lowercase letters. \n",
    "A common strategy is to reduce all letters in a word to lower case.\n",
    "It is very simple to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'gso',\n",
       " 'finace',\n",
       " 'group',\n",
       " 'in',\n",
       " 'u.s.a.',\n",
       " 'provided',\n",
       " 'cole',\n",
       " 'with',\n",
       " 'about',\n",
       " 'us$40,000,555.4',\n",
       " 'in',\n",
       " 'funding,',\n",
       " 'which',\n",
       " 'accounts',\n",
       " 'for',\n",
       " '35.3%',\n",
       " 'of',\n",
       " \"cole's\",\n",
       " 'revenue',\n",
       " '(i.e.,',\n",
       " 'aud113.3m),',\n",
       " 'as',\n",
       " 'the',\n",
       " 'asx-listed',\n",
       " 'firm',\n",
       " 'battles',\n",
       " 'for',\n",
       " 'its',\n",
       " 'survival.',\n",
       " 'mr.',\n",
       " 'johnson',\n",
       " 'said',\n",
       " \"gso's\",\n",
       " 'recapitalisation',\n",
       " 'meant',\n",
       " '\"the',\n",
       " 'current',\n",
       " 'shares',\n",
       " 'are',\n",
       " 'worthless\".']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token.lower() for token in tokens]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Removing Stop words\n",
    "Stop words are words that are extremely common and carry little lexical content. It is useful to remove stopwords in order to save storage space \n",
    "and speed up processing, and the process of removing these words is usually called “stopping.” \n",
    "An example stopword list from NLTK is shown bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list contains 127 stopwords in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'GSO',\n",
       " 'finace',\n",
       " 'group',\n",
       " 'U.S.A.',\n",
       " 'provided',\n",
       " 'Cole',\n",
       " 'US$40,000,555.4',\n",
       " 'funding,',\n",
       " 'accounts',\n",
       " '35.3%',\n",
       " \"Cole's\",\n",
       " 'revenue',\n",
       " '(i.e.,',\n",
       " 'AUD113.3m),',\n",
       " 'ASX-listed',\n",
       " 'firm',\n",
       " 'battles',\n",
       " 'survival.',\n",
       " 'Mr.',\n",
       " 'Johnson',\n",
       " 'said',\n",
       " \"GSO's\",\n",
       " 'recapitalisation',\n",
       " 'meant',\n",
       " '\"the',\n",
       " 'current',\n",
       " 'shares',\n",
       " 'worthless\".']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens = [token for token in tokens if token not in stopwords_list]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have removed 13 stopwords. The number of tokens left is 28. \n",
    "To check what stopwords have been excluded from the filtered list, you simply change `not in` to `in`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words accroding to NLTK's built-in stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "reuters = nltk.corpus.reuters.words()\n",
    "\n",
    "stopwords_list_570 = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords_list_570 = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735240435097661"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_reutuers = [w for w in reuters if w.lower() not in stopwords_list]\n",
    "len(filtered_reutuers)*1.0/len(reuters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words according to the downloaded stop word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6599769539328526"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_reutuers = [w for w in reuters if w.lower() not in stopwords_list_570]\n",
    "len(filtered_reutuers)*1.0/len(reuters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filtered out a lot of data by removing stop words.\n",
    "Stopwords usually appear to be of little value and have little impact on the final results, as the presence of stopwords in a text does not really help distinguishing it from other texts. \n",
    "\n",
    "Stopwords usually refer to the most common words in a language. \n",
    "The general strategy for determining whether a word is a stopword or not is to compute its total number of appearances in a corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Stemming and Lemmatization\n",
    "Another question in text pre-processing is whether we want to keep word forms like \"educate\", \"educated\", \"educating\", \n",
    "and \"educates\" separate or to collapse them. Grouping such forms together and working in terms of their base form is \n",
    "usually known as stemming or lemmatization.\n",
    "Typically the stemming process includes the identification and removal of prefixes, suffixes, and pluralisation, \n",
    "and leaves you with a stem.\n",
    "Lemmatization is a more advanced form of stemming.\n",
    "See Wikipedia entries for [stemming](https://en.wikipedia.org/wiki/Stemming) \n",
    "and [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation).\n",
    "\n",
    "The goal of stemming and lemmatization is to reduce either inflectional forms or derivational forms of \n",
    "a word to a common base form. \n",
    "\n",
    "NLTK provides several famous stemmers interfaces, such as\n",
    "\n",
    "* Porter Stemmer, which is based on \n",
    "[The Porter Stemming Algorithm](http://tartarus.org/martin/PorterStemmer/)\n",
    "* Lancaster Stemmer, which is based on \n",
    "[The Lancaster Stemming Algorithm](http://delivery.acm.org/10.1145/110000/101310/p56-paice.pdf?ip=130.194.73.168&id=101310&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E54DA4E88E6052E5D%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=586402953&CFTOKEN=41173049&__acm__=1456460730_26a9cd5f8f70e5d3e101f527c10e1a82),\n",
    "* Snowball Stemmer, which is based on [the Snowball Stemming Algorithm](http://snowball.tartarus.org/)\n",
    "\n",
    "Let's try the three stemmers on the words listed above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['watches', 'parties', 'carrying', 'loving', 'stopped', 'wetter', 'fattest', \n",
    "          'dying', 'darkness', 'agreement', 'friendship', 'derivational', 'denied',  'meeting']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porter Stemming Algorithm is the one of the most common stemming algorithms.\n",
    "It makes use of a series of heuristic replacement rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watches -> watch',\n",
       " 'parties -> parti',\n",
       " 'carrying -> carri',\n",
       " 'loving -> love',\n",
       " 'stopped -> stop',\n",
       " 'wetter -> wetter',\n",
       " 'fattest -> fattest',\n",
       " 'dying -> die',\n",
       " 'darkness -> dark',\n",
       " 'agreement -> agreement',\n",
       " 'friendship -> friendship',\n",
       " 'derivational -> deriv',\n",
       " 'denied -> deni',\n",
       " 'meeting -> meet']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "['{0} -> {1}'.format(w, stemmer.stem(w)) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Porter Stemmer works quite well on general cases, like 'watches' &#8594; 'watch' and 'darkness' &#8594; 'dark'.\n",
    "However, for some special cases, the Porter Stemmer might not work as expected, \n",
    "like  'carrying'  &#8594; 'carri' and 'derivational' &#8594; 'deriv'. \n",
    "Note that a concept called \"list comprehension\" supported by Python is used here.\n",
    "If you would like to know more about list comprehension, please click [here](http://www.secnetix.de/olli/Python/list_comprehensions.hawk).\n",
    "\n",
    "The Lancaster Stemmer is much newer than the Porter Stemmer, published in 1990."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watches -> watch',\n",
       " 'parties -> party',\n",
       " 'carrying -> carry',\n",
       " 'loving -> lov',\n",
       " 'stopped -> stop',\n",
       " 'wetter -> wet',\n",
       " 'fattest -> fattest',\n",
       " 'dying -> dying',\n",
       " 'darkness -> dark',\n",
       " 'agreement -> agr',\n",
       " 'friendship -> friend',\n",
       " 'derivational -> der',\n",
       " 'denied -> deny',\n",
       " 'meeting -> meet']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "['{0} -> {1}'.format(w, stemmer.stem(w)) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watches -> watch',\n",
       " 'parties -> parti',\n",
       " 'carrying -> carri',\n",
       " 'loving -> love',\n",
       " 'stopped -> stop',\n",
       " 'wetter -> wetter',\n",
       " 'fattest -> fattest',\n",
       " 'dying -> die',\n",
       " 'darkness -> dark',\n",
       " 'agreement -> agreement',\n",
       " 'friendship -> friendship',\n",
       " 'derivational -> deriv',\n",
       " 'denied -> deni',\n",
       " 'meeting -> meet']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "['{0} -> {1}'.format(w, stemmer.stem(w)) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stemmer usually resorts to language-specific rules. \n",
    "\n",
    "In NLTK, let's try lemmatizer in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watches -> watch',\n",
       " 'parties -> party',\n",
       " 'carrying -> carrying',\n",
       " 'loving -> loving',\n",
       " 'stopped -> stopped',\n",
       " 'wetter -> wetter',\n",
       " 'fattest -> fattest',\n",
       " 'dying -> dying',\n",
       " 'darkness -> darkness',\n",
       " 'agreement -> agreement',\n",
       " 'friendship -> friendship',\n",
       " 'derivational -> derivational',\n",
       " 'denied -> denied',\n",
       " 'meeting -> meeting']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "['{0} -> {1}'.format(w, lemmatizer.lemmatize(w)) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a bit strange that the lemmatizer did nothing to nearly all the words, except for 'watches', 'parties'\n",
    "However, if we specify the POS tag of each word, what will happen?\n",
    "Let try a couple of words in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dying', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meet'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('meeting', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meeting'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('meeting', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wet'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('wetter', pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fat'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('fattest', pos='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know the POS tags of the words, the WordNet Lemmatizer can accurately identify the corresponding lemmas.\n",
    "For example, the word 'meeting' with different POS tag, the WordNet Lemmatizer gives you different lemmas.\n",
    "Without giving the POS tags, it uses noun as default.\n",
    "\n",
    "Both stemming and lemmatization can significantly reduce the number of words in a vocabulary.\n",
    "In other words, the downstream text analysis tools can benefit from them by saving running time\n",
    "and memory space. In contrast, can stemming and lemmatization improve the performance\n",
    "of those tools? It is a quite arguable question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Sentence Segmentation\n",
    "\n",
    "Sentence segmentation is also known as sentence boundary disambiguation or sentence boundary detection.\n",
    "The following is the Wikipedia definition of sentence boundary disambiguation:\n",
    ">Sentence boundary disambiguation (SBD), also known as sentence breaking, is the problem in natural language processing of deciding where sentences begin and end. Often natural language processing tools require their input to be divided into sentences for a number of reasons. However sentence boundary identification is challenging because punctuation marks are often ambiguous. For example, a period may denote an abbreviation, decimal point, an ellipsis, or an email address - not the end of a sentence. About 47% of the periods in the Wall Street Journal corpus denote abbreviations. As well, question marks and exclamation marks may appear in embedded quotations, emoticons, computer code, and slang.\n",
    "\n",
    "SBD is one of the essential problems for many NLP tasks, like Parsing, Information Extraction, Machine Translation, and Document Summarizations. \n",
    "The accuracy of the SBD system will directly affect the performance of these applications. \n",
    "\n",
    "Sentences are the basic textual unit immediately above the word and phrase. \n",
    "So what is a sentence? Is something ending with one of the following punctuations \".\", \"!\", \"?\"?\n",
    "Does a period always indicate sentence boundaries?\n",
    "For English texts, it is almost as easy as finding every occurrence of those punctuations.\n",
    "However, some periods occur as part of abbreviations, monetary numerals and percentages.\n",
    "\n",
    "We will show you some sentence segmentation tools implemented in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the intruction on the official website of Punkt Sentence Tokenizer, we tokenize two snippets extracted\n",
    "from \"Moby Dick\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And so it turned out; Mr. Hosea Hussey being from home, but leaving \n",
      "Mrs. Hussey entirely competent to attend to all his affairs.\n",
      "-----\n",
      "Upon making known our desires \n",
      "for a supper and a bed, Mrs. Hussey, postponing further scolding for the present, ushered us \n",
      "into a little room, and seating us at a table spread with the relics of a recently concluded repast, \n",
      "turned round to us and said—\"Clam or Cod?\"\n"
     ]
    }
   ],
   "source": [
    "text1 = '''And so it turned out; Mr. Hosea Hussey being from home, but leaving \n",
    "Mrs. Hussey entirely competent to attend to all his affairs. Upon making known our desires \n",
    "for a supper and a bed, Mrs. Hussey, postponing further scolding for the present, ushered us \n",
    "into a little room, and seating us at a table spread with the relics of a recently concluded repast, \n",
    "turned round to us and said—\"Clam or Cod?\"'''\n",
    "print('\\n-----\\n'.join(sent_detector.tokenize(text1.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A clam for supper?\n",
      "-----\n",
      "a cold clam; is THAT what you mean, Mrs.\n",
      "-----\n",
      "Hussey?\"\n",
      "-----\n",
      "says I, \"but\n",
      "that's a rather cold and clammy reception in the winter time, ain't it, Mrs.\n",
      "-----\n",
      "Hussey?\"\n"
     ]
    }
   ],
   "source": [
    "text2 = '''A clam for supper? a cold clam; is THAT what you mean, Mrs. Hussey?\" says I, \"but\n",
    "that's a rather cold and clammy reception in the winter time, ain't it, Mrs. Hussey?\"'''\n",
    "print('\\n-----\\n'.join(sent_detector.tokenize(text2.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `sent_tokenize`, an instance of Punkt Sentence Tokenizer.\n",
    "This instance has already been trained on and works well for many European languages.\n",
    "```python\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    sent_tokenize(text1)\n",
    "```\n",
    "You should get similar outputs as above.\n",
    "\n",
    "Comparing the two results we notice that the sentence tokenizer has troubles in recognizing abbreviations.\n",
    "\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
